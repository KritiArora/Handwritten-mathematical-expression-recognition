{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "HMR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "v7RI63_x5nFR"
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import os, cv2\n",
        "from torchvision import models\n",
        "import torchvision.transforms as T\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j2yCHVhH5nFn"
      },
      "source": [
        "MAX_LENGTH = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jYktsHqC5nFw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "537fa2aa-50b6-4963-98bb-d139ef549743"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "03noI52R5nF_"
      },
      "source": [
        "Image_transform = T.Compose([T.Resize(256),\n",
        "                 T.CenterCrop(224),\n",
        "                 T.ToTensor(), \n",
        "                 T.Normalize(mean = [0.485, 0.456, 0.406], \n",
        "                             std = [0.229, 0.224, 0.225])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "qU3IDfB05nGK"
      },
      "source": [
        "def get_path(s):\n",
        "    paths = {\n",
        "        \"Images\" : \"data/images/\",\n",
        "        \"Vocab\" : \"data/kaggle/cleaned_mathml/vocab.csv\",\n",
        "        \"MathML\" : \"data/kaggle/cleaned_mathml/\"\n",
        "    }\n",
        "    return paths[s]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aHJgWkQD5nGa"
      },
      "source": [
        "Image_file_names = os.listdir(get_path(\"Images\"))\n",
        "MathML_file_names = os.listdir(get_path(\"MathML\"))\n",
        "\n",
        "temp_image = []\n",
        "temp_mm = []\n",
        "\n",
        "for mm_name in set(MathML_file_names):\n",
        "    img_name = mm_name[:-4] + \".inkml.png\"\n",
        "    if(img_name in Image_file_names):\n",
        "        temp_image.append(img_name)\n",
        "        temp_mm.append(mm_name)\n",
        "        \n",
        "Image_file_names = temp_image\n",
        "MathML_file_names = temp_mm\n",
        "data_size = len(Image_file_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JYdZkv9b5nGs"
      },
      "source": [
        "wordtoint = {\n",
        "    '<SOS>' : 0\n",
        "}\n",
        "inttoword = {\n",
        "    0 : '<SOS>',\n",
        "}\n",
        "with open(get_path(\"Vocab\")) as f:\n",
        "    words = f.read().strip().split(\" \")\n",
        "    for e, word in enumerate(words):\n",
        "        wordtoint[word] = e+1\n",
        "        inttoword[e+1] = word\n",
        "end = len(wordtoint)\n",
        "wordtoint['<EOS>'] = end\n",
        "inttoword[end] = '<EOS>'\n",
        "SOS_token = 0\n",
        "EOS_token = end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rFEYV9TH5nG5"
      },
      "source": [
        "def get_input_pair(index, EOS_token=EOS_token):\n",
        "    img = Image.fromarray(cv2.imread(get_path(\"Images\") + Image_file_names[index]))\n",
        "    img = Image_transform(img).unsqueeze(0)\n",
        "    tar = None\n",
        "    with open(get_path(\"MathML\") + MathML_file_names[index]) as f:\n",
        "        arr = f.read().strip().split(\" \")\n",
        "        tar = [torch.tensor([wordtoint[word]]) for word in arr]\n",
        "    tar.append(torch.tensor([EOS_token]))\n",
        "    tar = torch.tensor(tar).reshape(-1, 1)\n",
        "    return img, tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3lsFBQEAHey"
      },
      "source": [
        "img_data = None\n",
        "mm_data = None\n",
        "def get_input_pair_fast(index, img_data=img_data, mm_data=mm_data):\n",
        "    if(img_data != None):\n",
        "        return img_data[index], mm_data[index]\n",
        "    img_data = []\n",
        "    mm_data = []\n",
        "    for k in range(len(Image_file_names)):\n",
        "        a, b = get_input_pair(k)\n",
        "        img_data.append(a)\n",
        "        mm_data.append(b)\n",
        "    return img_data[index], mm_data[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JmjheCtq5nHB"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        temp_model = models.densenet169(pretrained=True)\n",
        "        modules = list(temp_model.children())[:-1]\n",
        "        self.densenet = nn.Sequential(*modules)\n",
        "\n",
        "        # It stops pytorch from precalulating gradients for pretrained model\n",
        "        for p in self.densenet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # (1, 1664, 7, 7)\n",
        "        self.cnn1 = nn.Conv2d(1664, 1500, kernel_size=(1,1))\n",
        "        self.cnn2 = nn.Conv2d(1500, 1400, kernel_size=(1,1))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, image):\n",
        "        # (1, 3, 224, 224)\n",
        "        output = self.densenet(image)\n",
        "        # (1, 1664, 7, 7)\n",
        "        output = self.relu(self.cnn2(self.relu(self.cnn1(output))))\n",
        "        # (1, 1400, 7, 7)\n",
        "        # (200, 343)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bm6y2i4f5nHM"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, drop_prob=0.1, max_size=MAX_LENGTH):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.drop_prob = drop_prob\n",
        "        self.max_size = max_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attn = nn.Linear(hidden_size * 2, max_size)\n",
        "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, inp, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(inp).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "#         print(attn_weights.shape)\n",
        "#         print(encoder_outputs.shape)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "        \n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        \n",
        "        return output, hidden, attn_weights\n",
        "        # (1, 1, 5)\n",
        "        # (1, 1, 1, 5)\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cUAub59j5nHZ"
      },
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bHw992zn5nHm"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, valid=False, max_length=MAX_LENGTH):\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    target_length = target_tensor.size(0)\n",
        "#     print(input_tensor.shape)\n",
        "#     print(target_tensor.shape)\n",
        "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    encoder_outputs = encoder(input_tensor).reshape(200, -1)\n",
        "    \n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "#     print(encoder_outputs.shape)\n",
        "    \n",
        "    decoder_hidden = decoder.initHidden()\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # print(target_tensor[di], decoder_output)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            \n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "    if(not valid):\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lUUZeuSE5nHz"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=5, learning_rate=0.001, e_start=1):\n",
        "    start = time.time()\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "    encoder_optimizer = optim.SGD([{\"params\" : encoder.cnn1.parameters()},\n",
        "                                   {\"params\" : encoder.cnn2.parameters()}]\n",
        "                                  , lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    last_valid_loss = None\n",
        "    data_index = list(range(data_size))\n",
        "    train_data_index = data_index[ : int(data_size*0.9)]\n",
        "    validation_data_index = data_index[int(data_size*0.9):]\n",
        "    if(len(train_data_index) + len(validation_data_index) != data_size):\n",
        "        print(\"partition Is Not Good Dude!!\")\n",
        "        return\n",
        "    for i in range(e_start, n_iters + 1):\n",
        "        random.shuffle(train_data_index)\n",
        "        total_loss = 0\n",
        "        encoder = encoder.train()\n",
        "        decoder = decoder.train()\n",
        "        for k in train_data_index:\n",
        "            input_tensor, target_tensor = get_input_pair(k)\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            target_tensor = target_tensor.to(device)\n",
        "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            total_loss += loss\n",
        "\n",
        "        loss_avg = total_loss/len(train_data_index)\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start, i / n_iters), i, i / n_iters * 100, loss_avg))\n",
        "        train_losses.append(loss_avg)\n",
        "        \n",
        "        valid_loss = 0\n",
        "        encoder = encoder.eval()\n",
        "        decoder = decoder.eval()\n",
        "        for k in validation_data_index:\n",
        "            input_tensor, target_tensor = get_input_pair(k)\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            target_tensor = target_tensor.to(device)\n",
        "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, valid=True)\n",
        "            valid_loss += loss\n",
        "\n",
        "        avg_valid_loss = valid_loss/len(validation_data_index)\n",
        "        valid_losses.append(avg_valid_loss)\n",
        "\n",
        "        if(last_valid_loss == None or (avg_valid_loss < last_valid_loss)):\n",
        "            last_valid_loss = avg_valid_loss\n",
        "            print(f'Saving Model at Epoch: {i} with validation loss {avg_valid_loss}')\n",
        "            save_state(i, encoder, encoder_optimizer, decoder, decoder_optimizer, avg_valid_loss)\n",
        "    showPlot(train_losses, valid_losses, 'red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "h0uAJbJ75nH8"
      },
      "source": [
        "def showPlot(points1, points2, color):\n",
        "    %matplotlib inline\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points1, color='red')\n",
        "    plt.plot(points2, color='blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpFFle8B4ZOm"
      },
      "source": [
        "# showPlot([1, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Z-Vu2oxD5nIE"
      },
      "source": [
        "def save_state(epoch, encoder, encoder_optimizer, decoder, decoder_optimizer, loss):\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'loss' : loss,\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
        "            'decoder_state_dict': decoder.state_dict(),\n",
        "            'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
        "            }, \"checkpoint\" + str(epoch) + \".pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yqHclr735nIM"
      },
      "source": [
        "# hidden_size = 256\n",
        "# encoder1 = Encoder().to(device)\n",
        "# attn_decoder1 = AttnDecoder(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "# rm -f *.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "luJ6BRIt5nIQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "e56136b1-0350-49d9-ce89-9a7f28a76ee3"
      },
      "source": [
        "# encoder = Encoder()\n",
        "# decoder = Decoder(343, len(wordtoint))\n",
        "# # model_states = torch.load('../input/modeltest/checkpoint7.pth', map_location=device)\n",
        "# # encoder.load_state_dict(model_states[\"encoder_state_dict\"])\n",
        "# # decoder.load_state_dict(model_states[\"decoder_state_dict\"])\n",
        "# trainIters(encoder, decoder, 10, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26m 17s (- 236m 38s) (1 10%) 2.4565\n",
            "Saving Model at Epoch: 1 with validation loss 2.198347059671399\n",
            "54m 15s (- 217m 2s) (2 20%) 2.1488\n",
            "Saving Model at Epoch: 2 with validation loss 1.897734227971344\n",
            "82m 2s (- 191m 26s) (3 30%) 2.0190\n",
            "109m 53s (- 164m 49s) (4 40%) 1.9215\n",
            "137m 52s (- 137m 52s) (5 50%) 1.8709\n",
            "Saving Model at Epoch: 5 with validation loss 1.7756718904327045\n",
            "165m 52s (- 110m 35s) (6 60%) 1.8255\n",
            "193m 50s (- 83m 4s) (7 70%) 1.7746\n",
            "221m 55s (- 55m 28s) (8 80%) 1.7612\n",
            "250m 3s (- 27m 47s) (9 90%) 1.7444\n",
            "278m 1s (- 0m 0s) (10 100%) 1.7193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nj6MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb3-fnbpYZS2"
      },
      "source": [
        "# def find_file():\n",
        "#     import re\n",
        "#     number = 1\n",
        "#     for filename in os.listdir():\n",
        "#         if(len(filename) > 5 and filename[-3:] == 'pth'):\n",
        "#             number = max(number, int(re.findall(r'\\d+', filename)[0]))\n",
        "#     return 'checkpoint' + str(number) + '.pth'\n",
        "# file_to_download = find_file()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KARVE8WYNCX"
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download(file_to_download)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF9FZ26IpK0k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "3fed972d-07b2-4895-c0d9-f9490ae3622a"
      },
      "source": [
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder(343, len(wordtoint)).to(device)\n",
        "model_states = torch.load('checkpoint5.pth', map_location=device)\n",
        "encoder.load_state_dict(model_states[\"encoder_state_dict\"])\n",
        "decoder.load_state_dict(model_states[\"decoder_state_dict\"])\n",
        "# trainIters(encoder, decoder, 10, 5)\n",
        "# trainIters(encoder, decoder, 10, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31m 44s (- 285m 40s) (1 10%) 1.6490\n",
            "Saving Model at Epoch: 1 with validation loss 2.313935104187453\n",
            "65m 22s (- 261m 28s) (2 20%) 1.6278\n",
            "99m 1s (- 231m 3s) (3 30%) 1.6304\n",
            "132m 31s (- 198m 46s) (4 40%) 1.6252\n",
            "165m 35s (- 165m 35s) (5 50%) 1.6415\n",
            "198m 16s (- 132m 10s) (6 60%) 1.6162\n",
            "231m 50s (- 99m 21s) (7 70%) 1.5953\n",
            "Saving Model at Epoch: 7 with validation loss 2.26041211835389\n",
            "265m 36s (- 66m 24s) (8 80%) 1.5881\n",
            "299m 16s (- 33m 15s) (9 90%) 1.5743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ8eJt7mvc1s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BNi7cIky5nIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "outputId": "d8a2ad8d-4423-4a2d-bf2b-23d5f5de391b"
      },
      "source": [
        "%matplotlib inline\n",
        "idx = 7\n",
        "img, tar = get_input_pair(idx)\n",
        "plt.imshow(cv2.imread(get_path(\"Images\") + Image_file_names[idx]))\n",
        "img = img.to(device)\n",
        "tar = tar.to(device)\n",
        "encoder = encoder.eval()\n",
        "decoder = decoder.eval()\n",
        "out = encoder(img).reshape(200, -1)\n",
        "hid = decoder.initHidden().to(device)\n",
        "start = torch.tensor([SOS_token]).to(device)\n",
        "with open(get_path(\"MathML\") + MathML_file_names[idx]) as f:\n",
        "    arr = f.read().strip().split(\" \")\n",
        "for i in range(len(arr)):\n",
        "    ans, hid, attn = decoder(start, hid, out)\n",
        "    print(inttoword[int(F.softmax(ans).argmax())],arr[i])\n",
        "    start = F.softmax(ans).argmax()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<mrow> <mrow>\n",
            "<mo> <msub>\n",
            "[ <mi>\n",
            "</mo> u\n",
            "<mrow> </mi>\n",
            "<mo> <mi>\n",
            "( n\n",
            "</mo> </mi>\n",
            "<mrow> </msub>\n",
            "<mi> <mrow>\n",
            "<mo> <mo>\n",
            "</mi> =\n",
            "</mrow> </mo>\n",
            "<mrow> <mrow>\n",
            "<mi> <mi>\n",
            "</mi> o\n",
            "</mrow> </mi>\n",
            "</mrow> <mrow>\n",
            "<EOS> <mo>\n",
            "</mi> (\n",
            "</mrow> </mo>\n",
            "</mrow> <mrow>\n",
            "</mrow> <msub>\n",
            "</mrow> <mi>\n",
            "</mrow> v\n",
            "</mrow> </mi>\n",
            "</mrow> <mi>\n",
            "</mrow> n\n",
            "</mrow> </mi>\n",
            "</mrow> </msub>\n",
            "</mrow> <mo>\n",
            "</mrow> )\n",
            "</mrow> </mo>\n",
            "</mrow> </mrow>\n",
            "</mrow> </mrow>\n",
            "</mrow> </mrow>\n",
            "</mrow> </mrow>\n",
            "</mrow> </mrow>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAB2CAYAAAAz69PvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAYZUlEQVR4nO3de3RU1dn48e8TQggkkQhCFiWIkKQo\nIC8KoqAipEHlpi0XqxR+tiLI8haqgMhbf60Ki/cHaaWu5UKD4AXra02pRcAFApIi0ldJUC4JItEX\nwXAJl3CJDYYk+/fHnBknkJBk5syc5MzzWeuszNlzZp/nbGYe9uzZ5xwxxqCUUspdopwOQCmllP00\nuSullAtpcldKKRfS5K6UUi6kyV0ppVxIk7tSSrlQSJK7iNwpIntFpEhEZodiH0oppeomds9zF5EW\nwFfAMOA7YBtwnzGm0NYdKaWUqlMoeu4DgCJjzDfGmArgHeDuEOxHKaVUHUKR3DsDB/3Wv7PKlFJK\nhUm0UzsWkanAVIC4uLh+V199tVOhKKVUs5Sfn3/cGNOhtudCkdyLgS5+68lWWQ3GmGwgG6B///4m\nLy8vBKEopZR7ici3dT0XimGZbUCaiHQTkRjgXuD9EOxHKaVUHWzvuRtjKkXkUWAd0AJYZowpsHs/\nSiml6haSMXdjzAfAB6GoWymlVP30DFWllHIhTe5KKeVCmtyVUqoOVVVVTocQME3uSqlLKisr4+WX\nX+bWW29FRHxL165dnQ4tpH744Qeio6MREQ4cOOB0OI3m2ElMSqmmr6SkhKSkJN/67bffTq9evRg/\nfjw9e/Z0MLLQa9WqFUuWLGHOnDm+/8hycnIYO3YsIuJwdPWz/cJhgdCTmJRqekpKSkhPT6egoIDk\n5GR27dpFYmKi02GF3alTp+jQoQOVlZUADBkyhE2bNjkclYeI5Btj+tf2nA7LKKVqlZSUREFBAUVF\nRRw8eDAiEztAYmIi58+fp7q6mu7du5Obm4uIsHjx4iY9Jq/JXSl1kbNnzwIQFRVFSkqKw9E0DSJC\nQUEB8+bNA+Dhhx/m+uuvdziqumlyV0rVcPbsWbp160ZUVBSFhXobBn+xsbHMmTMHYwzXX389O3fu\n9P3APGvWLPbs2eN0iD6a3JVSNcyfP58TJ04wYcIEevTo4XQ4TdZnn31Gdna2b33hwoX06dPHwYhq\n0uSulPI5dOgQ8+fPB2D58uUOR9O0tWjRgilTpmCMwRjDpEmTqKyspGXLlnz44YdOh6fJXSnlUVlZ\nyYQJEwAYMWKEw9E0P6+99hr3338/lZWV/OY3v3E6HE3uSimP2NhY/vnPf7JlyxbWrFnjdDjNTosW\nLXj99de57LLLOHToEKtXr3Y0Hk3uSingx1Ptb775Zocjad5eeOEFACZOnMi2bdsci0OTu1KKDz7w\nXKFbh2OC98ADD1BWVsbp06cZMGCAYwlek7tSyjfr46GHHnI4EneIi4vjtttuA2DYsGF89dVXYY9B\nry2jVIR7/vnnWblyJc888wx33XWX0+G4Rm5uLmVlZSQkJNCjRw/CfakX7bkrFeFeffVVACZPnuxw\nJO4THx/vu8jY3r17w7pvTe5KRTjv5Wzdfglfp3h7795hmnDR5K5UBDt06BAAP/nJTxyOxL3atGnD\nrbfeytGjR8O6X03uQRo9erTrr2ut3MkYQ0ZGBiLC1q1bnQ7H1Z599tmw71N/UA3CmDFjHD9RQalA\nFRQUsGfPHm655RYdkgmxvn37hn2fmtwDNHToUHJzc50OQ6mA3XPPPQD89a9/dTgS94uODn+q1WGZ\nAOTm5tZI7M3hlltKXch7eVodb3enoJK7iOwXkV0i8oWI5Fll7URkvYjss/5ebk+owRk8eDAiwvnz\n54Oua+jQob7Hy5cvp7q6Oug6VeQ5f/48ZWVldS7nzp0LeQzaMXEvO3ruQ40xff3u4zcb2GiMSQM2\nWuuOMMbQqVMnRISPP/6YlJQUWrZsGVSdFw7FTJw4Maj6VGQSEWJiYkhISKhzmTJlSsj2v3btWgCm\nT58esn00VFpaGsnJyb7l3XffdTokVwjFQNDdwBDr8RtALvBUCPZzScePH2fcuHEcOXIE8JyFN2PG\njKDr9f/V24lfwFXz5z/GfdNNNxEfH1/rdm+99RaHDx+uUZaQkMCKFSuIigquX5afnw9A//613ls5\nYIWFhWRlZZGQkMAzzzzDFVdcUe9rioqKaqz/8pe/BGDcuHFBH2dE815oPpAF+F9gO5APTLXKTvk9\nL/7rF7x2KpAH5F155ZXGbo899pgBzOzZs22rc+DAgQYwgJk5c6Zt9UaqGTNmOB1C2D344IMGMCNH\njjR79uy55Lbe91ptS4sWLYKKo0uXLiYhIcFUVVUFVY/X3LlzTXx8/EVxNkR0dPQljzXY5Z577rHl\nGIPV0PZoZJ15pq78XNcTDVmAztbfjsAOYPCFyRwora+efv362X7QV199tQHMoUOHbKvT+2bp06eP\nqaiosK3eSHTu3DkDmBkzZpijR486HU7YeBNZaWlpvduWlZVdtKxZs8b3PszJyQk4DsAMHjw44Nd7\nVVVVmTfeeMMXU3x8vJk7d26jkvu5c+d8x3fmzBkzZcoUTe4NrzM0yb1GRfAHYAawF+hklXUC9tb3\n2lAk98a8uRri2LFjttcZ6TIzM31t2rp164joydvxHqqqqjL9+/c3gHnrrbcCjiM7OzvgGAoLC82o\nUaN8xzNq1ChTUFBQo347jtNbz/r164Oqy2nr1q0Le3IPeEBLROJEJMH7GLgd2A28D9xvbXY/sDLQ\nfTQVxhi9UXAILFq0iMzMTGJjYykvLycrK4uZM2dSUlISUH0VFRW88sorxMXFXbTcfffdNkfvnKio\nKN9YdmJiYsD19OrVK6DXnThxgmuvvdZ3At91113HqlWrbD9TOyoqyncJ4pkzZ4b9qop28v7GEVZ1\nZf36FqA7nqGYHUAB8J9WeXs8s2T2ARuAdvXV1dR77lFRUb76Tp06ZUudqqZDhw5d1JMfN26cmTBh\ngrnzzjsb9TW8R48epnfv3r4lMTHRACY9Pd3RY3z//fcNYDIzM4Oqx9tjXrx4cUCvr66uDvqzER8f\nb+bNm1fn83Z+/rztdu+999pSnxOuuuoqExcXZ3u9XKLnHvBsGWPMN8B/1FJ+AvhZoPXawdvz6969\ne9B1ff7557557KNHj6Zt27ZB16ku1qlTJxYtWgTAK6+8Qnl5OX/7298u2q5FixZ1zs2Ojo4mIyOD\nVatW1SgvKipi6NCh/PnPf7Y/8Eb44YcfAOjSpUtQ9Xh7zIHehDnQb0b+ioqKSEpKCrqehrjxxhsB\n+Pe//x2W/dntwIED7N+/P+y3L3Tl5Qe8c89zcnKCqqd9+/acPHnSt75gwYKg6lP1W7RoEYsWLaKs\nrMx3Fb2WLVty5ZVXBlxnamoqBw8etCtEx5SUlJCeng54jqlVq1YB1fPyyy8HHUt9iT0hIYGzZ8/y\n7bffBn3dmnXr1gH2dNacMGjQIADeeeedsO7XlZNIDxw4QHx8PNddd11QdXgT+5gxYwA9my+c4uPj\nSUlJISUlJajE3hSZAMeO09PTKSgoIDk52XcSUiDCcU2kxx9/HICsrKyg6/Ke1DR27Nig63JCcXEx\nAMnJyWHdryuTu/cHOv9ed2O8+uqrdO3alc6dO1NaWkpCQoLNEapINGrUKADmzZvXqNd9+eWX9OrV\ny5fYDx48SEpKSkAxbN68mdzc3ICHdBpq7ty5iAiffvppUPXMnz+f1atX8/jjj3PLLbfYFF34eG+E\n4kQHxZXJPSMjg6qqKv7xj38E9PonnngCgCVLlpCYmMiZM2cAz3ivUoGKjY0F4NSpUw1+TVVVFX37\n9qWwsJDk5OSge93bt28HCEuijImJYf/+/UHV4e35P/zwwzZEFH5/+ctfAHjwwQfDv/O6fmkN52L3\nbBnvbIDo6OhGve67774zqampBjApKSm+cnR+u7JJQ95L//rXv8zkyZN924qIefLJJ23Z/4IFCwxg\nVqxYYUt9l7Jz504DmI8//jig1585c8YAJioqyubIwmPfvn0GMDfccEPI9kEo5rk3Zd6x8crKyka9\nbsiQIRQVFdG9e3c2bdoUitCUAuCll17yzZ650MCBA1m6dKlvfcuWLbaMXYfbtddeC8Dw4cPZsmVL\no1//0ksvATBhwgRb4woX74SO8ePHOxNAXVk/nEtTmOfu7bF379496LqUqsuuXbvMXXfd5XtPffTR\nR2b69OkXzdWfPn16SPYfzp67McaUl5f7jqkxPfiRI0cawEycODGE0YVWOPIGkdZz91daWlrvNsXF\nxb4eu5N3V9Lrwrtf7969Wblype8M0/T0dN/8fq8nnniCP/3pT06EZ7vY2FjuuOMOwNODf/HFF+t9\nzb333suaNWsA+N3vfteg/VRVVQUepEu5cp47eE6yGDRoEO3ataNfv37k5eXVul1aWpovsX/99ddh\njvJHs2bNYuHChbz33nv8/Oc/dyyO5mL16tW89957Ddr2V7/6lW9+eFNx7NgxCgoKmD17NmPHjuXX\nv/610yGFzNq1azl37hytW7cmMzOTzMxMOnXqxPDhw2ts99prr/mmiWZlZTFt2jTi4uLqrf+hhx4i\nOzub9evXk5GREZJjaKzs7GwARowY4VgMrk3uKSkpbNq0iV69epGfn09paSmXX/7jTaF2797Nc889\n57uWtNP3Q7355ptZuHAhn3zySUQn9/Lycs6cOUNxcTFnz56luLiYM2fOcPbsWcAzjFhQUMCbb77Z\n4DpzcnI4cuQIbdq0CVXYAenVq9dFZ9O6VWxsLF988QXPPfccf//73zl8+DDLli2rddusrCyefPLJ\nBtc9ZMgQsrOzyc3NbRLJvaKiwjfd9fe//71jcYj3f0on9e/f39TVsw7W1q1bfaf9Zmdns2HDBnJy\ncnw9hD/+8Y9Mmzbtkh987w+0wbRVRUUFBw4cIDY2ts6TGUSEli1bUlFREfB+mjI7TwKbOHEiTz/9\ndL3bjR8/nsLCQg4ePBj2k0iaooULFzJr1ixWrFjhOznPCUePHuXEiRM1ylJTU4mJiQmovvo+OyUl\nJZw+fRqAjh07hvQyIl27duXAgQN8+umnDBgwIGT7ARCRfPPjXfBqcG3P3WvAgAG0b9+eEydOMHXq\nVF95x44deeqpp3xz2kOprKyMbt26cfz4cYYOHcpHH31U57Z23OO1OUpKSuKyyy6jc+fOJCQk0Llz\n5xrr/h/Gtm3b8rOfNezyRcuWLaO4uJj27duHKnQVgKSkJNuvTVPXZ2fatGlkZ2f7OmetW7dmwYIF\nPProo7bu38t74lKoE3t9XJ/co6OjOX78OFu2bOHtt9+mZ8+ePPDAAyH/iv7UU0+xdOlSX+9ERMjI\nyGhyY7/h5MS3RO9Fp5S7xcTEUFFRQXV1te/WfLfddhubN2/2bTNp0iTAc1P7xx57jLi4ONvP1G0K\nY+0+dU2jCecSiqmQdsKa0lRdXV3vtmlpab7tW7VqZXJycsz333/foP2MHj3aAKawsDDYkJWq1dat\nWw1ghg8f7nQotlq+fLkBzJw5c0xZWZkZPHiwAUzbtm1r3T4uLs506NDB1hi8d6AaMWKErfVeCpE8\nFdIO7dq1Azw3JajrSpNlZWX89re/Zd++fYDnwknffPMN48aNa/C3BO9vA5988okNUSt1sRtvvJEu\nXbqwYcMGp0Ox1U9/+lPA8zlcunQpmzdvpm3btnUeZ8eOHTl27Biff/65bTE8++yzADz//PO21RmU\nurJ+OJem3nM3xtS4d+WllqysLFNZWRnQPsrLy01MTIxJS0uzOXqlfrRt2zYDuOo+wKWlpaZNmzYG\nMGPHjjWA2b59e53bey9tgE0nGXlPDuvZs6ct9TUU4biHajBLc0juxhizdu1aM2nSpFqTuoiY++67\nL+h9DBo0yAARddNoFX6Aef31150Ow1bvvvtujc9kSUnJJbe3K7mfPn3atGvXzkRFRZnPPvss6Poa\n41LJXYdlGuGOO+7gzTffrLUhq6urefvtt4Pex6xZswDPlDWlQmnjxo1Oh2Cr8ePH17i0bocOHcKy\n327dunHy5Em+/PJLbrjhhrDssyE0uTcx3ru21HZD3T/84Q888sgjPPLII5SXl4c7NOUyH374IV99\n9ZXTYdiqIec+2M1734i0tLSw7/tSXH8SU3NjjCEqKoqBAweydetWX/nJkycvmqudkZHB+vXrwx2i\ncgH/91NTyAF2auhJh97ttm7dysCBAwPeD3jm0i9evLjRdQTrUicxac+9GTh58iTDhg0DIC8vjyVL\nlgCwYcMG7cGrgHhngIHnOjdusmPHjrBeSGzMmDENuiBauLn+JKbmzr+HVVhYyDXXXEO/fv3o2bMn\nw4YNIykpyXenKKUaY9++ffziF78I29h0uPTp06dB2wX7jaWpf+PRnnsTVlZW5uuxJyUlcc011/ie\nGzRoEKmpqY2+IYlSXqmpqezYscPpMFSI1JvcRWSZiJSIyG6/snYisl5E9ll/L7fKRUReFJEiEdkp\nIteHMni369y5M9u3b2fXrl0cOXLkoudHjhxJeXl5yK6RodzPe6q+cp+G/Mu+Dtx5QdlsYKMxJg3Y\naK0DDAfSrGUqEP5fGFyioqLCN9zSu3fvWrfxv5G3Ukr5qze5G2M2AycvKL4beMN6/Abwc7/yN635\n9f8DJIpIJ7uCjST5+fm0bt36kqeJe+/m49ZLBCulAhfod7IkY8xh6/ERwHvtzs7AQb/tvrPKVABW\nrVrV4EvbKqWUv6AH3KxTYBv9s7GITBWRPBHJc9tUrGD4/wLfkMSekJAAaO9dKVVToMn9qHe4xfpb\nYpUXA138tku2yi5ijMk2xvQ3xvR321SsYHh/4PJehqA+o0ePBmDbtm0hi0kp1fwEOs/9feB+4L+s\nvyv9yh8VkXeAG4HTfsM3qoEaM382NTUVgO+//z5U4SilmqF6k7uI/DcwBLhCRL4Dfo8nqb8rIpOB\nb4F7rM0/AEYARcC/AXtvc6KUUqpB6k3uxpj76njqogFha/z9kWCDUkopFRw9g0EppVxIk7tSSrmQ\nJnellHIhTe5KKeVCerMOpZRqpvRmHUopFWE0uSullAtpcldKKRfS5K6UUi6kyV0ppVxIk7tSSrmQ\nJnellHIhTe5KKeVCTeIkJhE5C+x1Og6HXQEcdzoIh2kbaBuAtgE0vA26GmNqvdtRoDfrsNveus6y\nihQikqdtoG2gbaBtAPa0gQ7LKKWUC2lyV0opF2oqyT3b6QCaAG0DbQPQNgBtA7ChDZrED6pKKaXs\n1VR67koppWzkeHIXkTtFZK+IFInIbKfjCRURWSYiJSKy26+snYisF5F91t/LrXIRkRetNtkpItc7\nF7l9RKSLiGwSkUIRKRCRTKs8YtpBRGJF5DMR2WG1wbNWeTcR+dQ61r+KSIxV3spaL7Kev8rJ+O0k\nIi1E5HMRWW2tR1QbiMh+EdklIl+ISJ5VZttnwdHkLiItgJeA4UBP4D4R6elkTCH0OnDnBWWzgY3G\nmDRgo7UOnvZIs5apwOIwxRhqlcCTxpiewE3AI9a/dyS1ww9AujHmP4C+wJ0ichPw/4AXjDGpQCkw\n2dp+MlBqlb9gbecWmcAev/VIbIOhxpi+ftMe7fssGGMcW4CBwDq/9aeBp52MKcTHexWw2299L9DJ\netwJz3x/gFeA+2rbzk0LsBIYFqntALQBtgM34jlhJdoq930ugHXAQOtxtLWdOB27DceebCWvdGA1\nIBHYBvuBKy4os+2z4PSwTGfgoN/6d1ZZpEgyxhy2Hh8BkqzHrm8X66v1dcCnRFg7WMMRXwAlwHrg\na+CUMabS2sT/OH1tYD1/Gmgf3ohDYhEwC6i21tsTeW1ggA9FJF9Eplpltn0WmsoZqhHPGGNEJCKm\nLolIPLACmG6MOSMivucioR2MMVVAXxFJBN4DrnY4pLASkVFAiTEmX0SGOB2Pg24xxhSLSEdgvYh8\n6f9ksJ8Fp3vuxUAXv/VkqyxSHBWRTgDW3xKr3LXtIiIt8ST2vxhj/m4VR1w7ABhjTgGb8AxBJIqI\nt7Plf5y+NrCebwucCHOodrsZuEtE9gPv4Bma+TOR1QYYY4qtvyV4/pMfgI2fBaeT+zYgzfqVPAa4\nF3jf4ZjC6X3gfuvx/XjGoL3l/8f6hfwm4LTfV7VmSzxd9KXAHmPMn/yeiph2EJEOVo8dEWmN5zeH\nPXiS/DhrswvbwNs244CPjDXo2lwZY542xiQbY67C85n/yBjzKyKoDUQkTkQSvI+B24Hd2PlZaAI/\nKowAvsIz7vifTscTwuP8b+AwcB7PeNlkPOOGG4F9wAagnbWt4JlF9DWwC+jvdPw2tcEteMYZdwJf\nWMuISGoHoA/wudUGu4H/a5V3Bz4DioAcoJVVHmutF1nPd3f6GGxujyHA6khrA+tYd1hLgTf32flZ\n0DNUlVLKhZwellFKKRUCmtyVUsqFNLkrpZQLaXJXSikX0uSulFIupMldKaVcSJO7Ukq5kCZ3pZRy\nof8PiOraasSEmG4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "u0gNl39-5nIl"
      },
      "source": [
        "['<mrow>',\n",
        " '<mi>',\n",
        " 's',\n",
        " '</mi>',\n",
        " '<mrow>',\n",
        " '<mo>',\n",
        " '(',\n",
        " '</mo>',\n",
        " '<mrow>',\n",
        " '<mi>',\n",
        " 'u',\n",
        " '</mi>',\n",
        " '<mrow>',\n",
        " '<mo>',\n",
        " ')',\n",
        " '</mo>',\n",
        " '<mrow>',\n",
        " '<mo>',\n",
        " '=',\n",
        " '</mo>',\n",
        " '<mfrac>',\n",
        " '<mrow>',\n",
        " '<mi>',\n",
        " 'sin',\n",
        " '</mi>',\n",
        " '<mrow>',\n",
        " '<mo>',\n",
        " '(',\n",
        " '</mo>',\n",
        " '<mrow>',\n",
        " '<mi>',\n",
        " 'u',\n",
        " '</mi>',\n",
        " '<mo>',\n",
        " ')',\n",
        " '</mo>',\n",
        " '</mrow>',\n",
        " '</mrow>',\n",
        " '</mrow>',\n",
        " '<mrow>',\n",
        " '<mi>',\n",
        " 'sin',\n",
        " '</mi>',\n",
        " '<mrow>',\n",
        " '<mo>',\n",
        " '(',\n",
        " '</mo>',\n",
        " '<mrow>',\n",
        " '<mi>',\n",
        " 'lambda',\n",
        " '</mi>',\n",
        " '<mo>',\n",
        " ')',\n",
        " '</mo>',\n",
        " '</mrow>',\n",
        " '</mrow>',\n",
        " '</mrow>',\n",
        " '</mfrac>',\n",
        " '</mrow>',\n",
        " '</mrow>',\n",
        " '</mrow>',\n",
        " '</mrow>',\n",
        " '</mrow>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "n08NlsYN5nIv"
      },
      "source": [
        "# decoder_hidden = decoder.initHidden()\n",
        "# inp_tar = torch.tensor([0])\n",
        "# for i in range(len(exp_tar)+1):\n",
        "#     decoder_output, decoder_hidden, decoder_attention = decoder(inp_tar, decoder_hidden, out)\n",
        "#     if(i == len(exp_tar)):\n",
        "#         break\n",
        "#     inp_tar = exp_tar[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WcYMMbYx5nI3"
      },
      "source": [
        "# ans = 0\n",
        "# for i in range(data_size):\n",
        "#     img, tar = get_input_pair(i)\n",
        "#     ans = max(ans, len(tar))\n",
        "# ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9c5PW9q05nI8"
      },
      "source": [
        "# len(wordtoint)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vtZ_yw5E5nJA"
      },
      "source": [
        "# !unzip \"/content/drive/My Drive/data (1)/cleaned_mathML.zip\" -d data/\n",
        "# !unzip \"/content/drive/My Drive/data (1)/hme_images.zip\" -d data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpxLIp203KnQ"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgw9QPkX3LZA"
      },
      "source": [
        "# !unzip \"/content/drive/My Drive/data (1)/cleaned_mathML.zip\" -d data/\n",
        "# !unzip \"/content/drive/My Drive/data (1)/hme_images.zip\" -d data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPHGU5hEDJAw"
      },
      "source": [
        "# teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "# def train2(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, valid=False, max_length=MAX_LENGTH):\n",
        "#     encoder_optimizer.zero_grad()\n",
        "#     decoder_optimizer.zero_grad()\n",
        "\n",
        "#     target_length = target_tensor.size(0)\n",
        "# #     print(input_tensor.shape)\n",
        "# #     print(target_tensor.shape)\n",
        "# #     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "#     loss = 0\n",
        "\n",
        "#     encoder_outputs = encoder(input_tensor).reshape(200, -1)\n",
        "    \n",
        "#     decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "# #     print(encoder_outputs.shape)\n",
        "    \n",
        "#     decoder_hidden = decoder.initHidden()\n",
        "\n",
        "#     use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "#     loss = decode_data(target_length, decoder_input, decoder_hidden, encoder_outputs)\n",
        "    \n",
        "#     if(not valid):\n",
        "#         loss.backward()\n",
        "\n",
        "#         encoder_optimizer.step()\n",
        "#         decoder_optimizer.step()\n",
        "\n",
        "#     return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rli3Y-2cR-i6"
      },
      "source": [
        "# def decode_data(target_length, decoder_input, decoder_hidden, encoder_outputs):\n",
        "#     loss = 0\n",
        "#     if use_teacher_forcing:\n",
        "#         # Teacher forcing: Feed the target as the next input\n",
        "#         for di in range(target_length):\n",
        "#             decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "#             # print(target_tensor[di], decoder_output)\n",
        "#             if(int(F.softmax(ans).argmax()) != int(target_tensor[di])):\n",
        "#                 loss += criterion(decoder_output, target_tensor[di])\n",
        "#                 break\n",
        "#             decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "#     else:\n",
        "#         # Without teacher forcing: use its own predictions as the next input\n",
        "#         for di in range(target_length):\n",
        "#             decoder_output, decoder_hidden, decoder_attention = decoder( decoder_input, decoder_hidden, encoder_outputs)\n",
        "#             topv, topi = decoder_output.topk(1)\n",
        "#             decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "#             loss += criterion(decoder_output, target_tensor[di])\n",
        "#             if decoder_input.item() == EOS_token:\n",
        "#                 break\n",
        "#     return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd-SlkKeGqBJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}